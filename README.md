## 作业：构建两层神经网络分类器
##### 本项目没有使用pytorch，tensorflow等python package，使用了numpy
### 一、模型训练步骤如下：
#### 1. 激活函数的选择：
本项目尝试选用激活函数，经测试，最终在 Sigmoid函数、ReLU函数、Leaky ReLU函数、Tanh函数中选用了 ReLU函数，ReLU函数将负数的输出值设为0，正数的输出值不变，具有简单、快速和有效的优点。此外，本项目为多分类问题，故定义了 softmax函数，用于其输出层。
#### 2. 数据加载与参数初始化：
本项目使用MNIST数据集，对手写数字进行识别，为多分类问题，共存在0~9共十种可能的分类结果。项目在开始阶段，加载了MNIST数据集，并分别定义了训练集与测试集。  

两层神经网络共包含一个隐藏层和一个输出层，需要两组参数，故本项目定义了w1、b1和w2、b2两组参数，并根据数据集的形状对其进行了初始化。
#### 3. 损失函数的定义：
本项目选用了交叉熵损失函数并对其进行了定义。
#### 3. 前向传播、反向传播、L2正则化以及梯度的计算：
训练模型的函数采用了全局变量（global）w1, b1, w2, b2来记录网络的权重和偏置，并且传入输入数据x和对应的真实标签y_true。  

函数首先通过前向传播计算得到网络输出output_layer，然后使用交叉熵损失函数与L2正则化计算损失loss，并计算损失关于网络输出的梯度delta_output。接着，根据反向传播算法，计算损失关于每个权重和偏置的梯度。
#### 4. 学习率下降策略和优化器SGD：
训练模型的函数在计算梯度后根据学习率、学习率下降策略以及正则化参数更新网络参数w1, b1, w2, b2，使得网络的输出更加接近真实标签，同时避免过拟合。最后，训练模型返回损失值loss。其中，这里的学习率下降策略通过定义函数learning_rate_decay来动态调整学习率。  

最后，保存该训练模型用于测试。

### 二、参数查找：
经手动测试，最终选用的参数如下：
###### 学习率：learning_rate = 1e-4
###### 隐藏层大小：hidden_dim = 100
###### 正则化强度：reg_lambda = 1e-5
###### 学习率下降速率：decay_rate = 1e-1
###### 训练次数：num_epochs = 20
###### 批次大小：batch_size = 16
### 三、测试步骤如下：
#### 1、预测函数与accuracy记录：
预测函数中使用了 ReLU函数和 softmax函数，最后，运用 numpy库的 argmax函数返回矩阵中每一行中最大元素的索引，即类别标签。  

accuracy_score来自于 scikit-learn机器学习库，用于计算分类模型的准确率。
#### 2、测试结果的记录
使用前期所选参数与训练好的模型进行测试，在每个 epoch结束后测试模型并记录 loss和 accuracy，记录了模型训练与测试的过程，最终得到一个较好的模型（Test accuracy: 0.938）。  

并绘制训练和测试的 loss曲线，测试的 accuracy曲线，以及可视化每层的网络参数。
